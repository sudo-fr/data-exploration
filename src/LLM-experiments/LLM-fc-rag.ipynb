{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  /$$$$$$  /$$   /$$ /$$$$$$$   /$$$$$$ \n",
    " /$$__  $$| $$  | $$| $$__  $$ /$$__  $$\n",
    "| $$  \\__/| $$  | $$| $$  \\ $$| $$  \\ $$\n",
    "|  $$$$$$ | $$  | $$| $$  | $$| $$  | $$\n",
    " \\____  $$| $$  | $$| $$  | $$| $$  | $$\n",
    " /$$  \\ $$| $$  | $$| $$  | $$| $$  | $$\n",
    "|  $$$$$$/|  $$$$$$/| $$$$$$$/|  $$$$$$/\n",
    " \\______/  \\______/ |_______/  \\______/ \n",
    "\n",
    "\n",
    " @Author : Pierre Lague\n",
    "\n",
    " @Email : p.lague@sudogroup.fr\n",
    "\n",
    " @Date : 04/10/2024\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM - Function Calling and Retrieval Augmented Generation\n",
    "\n",
    "This notebook aims to display the basics of FC and RAG for LLM use in a local environement.\n",
    "\n",
    ">N.B. User should have Ollama installed\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is an AI technique that enhances the capabilities of large language models by combining them with external knowledge retrieval. In RAG, when a query is received, relevant information is first retrieved from a knowledge base. This retrieved context is then provided to the language model along with the original query, allowing the model to generate a response that's informed by both its pre-trained knowledge and the specific, relevant information from the external source. This approach helps to ground the model's responses in factual information, reduce hallucinations, and provide more up-to-date or domain-specific answers.\n",
    "\n",
    "![alt-text](./assets/schema_RAG.webp)\n",
    "\n",
    "**Function calling**, in the context of large language models, refers to the ability of these models to recognize when a specific task or query requires the execution of an external function, and to call or recommend calling that function appropriately. Instead of trying to generate the answer itself, the model identifies that the query maps to a particular function and outputs a structured request to call that function. This capability allows language models to integrate more seamlessly with external tools, APIs, or data sources, extending their functionality beyond mere text generation. For instance, if asked about the current time, a model with function calling capabilities might recognize that it needs to call a time-retrieval function rather than trying to guess the time based on its training data.\n",
    "\n",
    "![alt-text](./assets/llm-function-calls.webp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_knowledge_base(file_path):\n",
    "    \"\"\"Loads the knowledge base for the model to retrieve information and context.\n",
    "\n",
    "    Args:\n",
    "        file_path (string): filepath to the knowledge base\n",
    "\n",
    "    Returns:\n",
    "        the content of the knowledge base.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().split('\\n\\n')  # Assuming paragraphs are separated by blank lines\n",
    "\n",
    "def query_model(prompt, model=\"tinyllama\"):\n",
    "    \"\"\"Queries the model given in parameter with a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (string): prompr from which the model receives instructions on how to handle it's knowledge base. \n",
    "        model (str, optional): Name of the Ollama model. Defaults to \"tinyllama\".\n",
    "\n",
    "    Returns:\n",
    "        string : the response from the model\n",
    "    \"\"\"\n",
    "    response = requests.post(OLLAMA_API, json={\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    })\n",
    "    return response.json()['response']\n",
    "\n",
    "def retrieve_context(query, knowledge_base, top_n=3):\n",
    "    \"\"\"This function retrieves context from the knowledge base.\n",
    "    It starts by vectorizing the content and computing the similarities bewteen the the content of the KB\n",
    "    and the vectorized query. Getting the top_n best similarities will form the answer.\n",
    "\n",
    "    Args:\n",
    "        query (str): the query for the model\n",
    "        knowledge_base (file/str): the KB containing context\n",
    "        top_n (int, optional): how much of the top similarities we use and return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        the content of the KB that most corresponds to our query based on cosine similarity.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    kb_vectors = vectorizer.fit_transform(knowledge_base)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, kb_vectors)[0]\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    return \"\\n\".join([knowledge_base[i] for i in top_indices])\n",
    "\n",
    "def rag_query(query, knowledge_base, top_n):\n",
    "    context = retrieve_context(query, knowledge_base, top_n=top_n) #getting the context\n",
    "    \n",
    "    # this is called prompt engineering, guiding the LLM to act how we wish by providing it clear instructions.\n",
    "    prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Based on the context provided, please answer the query. If the context doesn't contain relevant information, say so and provide a general answer based on your knowledge of FinOps and cloud computing.\"\"\"\n",
    "    \n",
    "    return query_model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinOps and Cloud Computing Q&A System\n",
      "Enter 'quit' to exit\n",
      "\n",
      "Response: In addition to providing cloud services and optimizing infrastructure, FinOp strategies can help businesses achieve cost optimization by automating tasktags, integrating machine learning, and adopting more sophisticated solutions like IA. While continuously evolving in a constant pace, FinOps provides opportunities for adapting quickly to changes and implementing culture of optimizing team-wide. The combination of proactive methodologies, a well-strategized approach, and cost transformation as the underlying motivator can lead to significant cost savings for businesses.\n"
     ]
    }
   ],
   "source": [
    "kb_file_path = os.path.join(\"./data/finops_cloud_kb.txt\")\n",
    "knowledge_base = load_knowledge_base(kb_file_path)\n",
    "\n",
    "print(\"FinOps and Cloud Computing Q&A System\")\n",
    "print(\"Enter 'quit' to exit\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nEnter your query: \")\n",
    "    if query.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    response = rag_query(query, knowledge_base, top_n=10)\n",
    "    print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RAG Implementation in the Script:\n",
    "\n",
    "1. Knowledge Base: We have a simple in-memory knowledge base represented by the `knowledge_base` which is a text file containing information about the cloud and FinOps. In a real-world scenario, this could be a much larger database or document store.\n",
    "\n",
    "2. Retrieval Function: The `retrieve_context` function implements a basic retrieval mechanism. It searches the knowledge base for sentences containing the query and concatenates them.\n",
    "\n",
    "3. RAG Process: The `rag_example` function ties it all together:\n",
    "   - It first calls `retrieve_context` to get relevant information.\n",
    "   - It then constructs a prompt that includes both the retrieved context and the original query.\n",
    "   - Finally, it sends this enriched prompt to the language model via the `query_model` function.\n",
    "\n",
    "This implementation allows the model to generate responses based on both its pre-trained knowledge and the specific information retrieved from the knowledge base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
